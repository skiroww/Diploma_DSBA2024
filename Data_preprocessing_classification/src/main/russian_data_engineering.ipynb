{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b389e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from navec import Navec\n",
    "path = r\"C:\\Users\\User\\Documents\\github\\He_who_laughs_last\\data\\embedded\\navec.tar\"\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c21b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_read_russia(jk):\n",
    "    res = []\n",
    "    for i in tqdm(jk):\n",
    "        clean_text = re.sub(r'(?:(?!\\u0301)[\\W\\d_])+', ' ', i)\n",
    "        res.append(clean_text.split())\n",
    "    return res\n",
    "\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "def pad_data(data,target,mode='embed'):\n",
    "    if mode !='embed':\n",
    "        for i in tqdm(range(len(data))):\n",
    "            if(len(data[i]) == target):\n",
    "                continue\n",
    "            elif(len(data[i]) < target):\n",
    "                while len(data[i]) != target:\n",
    "                    data[i].append(0)\n",
    "            elif(len(data[i]) > target):\n",
    "                data[i] = data[i][:target]\n",
    "        return data\n",
    "    else:\n",
    "        for i in tqdm(range(len(data))):\n",
    "            if(len(data[i]) == target):\n",
    "                continue\n",
    "            elif(len(data[i]) < target):\n",
    "                while len(data[i]) != target:\n",
    "                    data[i].append(navec['<pad>'])\n",
    "            elif(len(data[i]) > target):\n",
    "                data[i] = data[i][:target]\n",
    "        return data\n",
    "\n",
    "def embed_navec(data):\n",
    "    embedded = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        emb = []\n",
    "        for word in data[i]:\n",
    "            if(navec.get(word) is None):\n",
    "                emb.append(navec['<unk>'])\n",
    "            else:\n",
    "                emb.append(navec[word])\n",
    "        embedded.append(emb)\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1811ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jokes_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m jokes_raw[jokes_raw\u001b[39m.\u001b[39mlength \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m]\u001b[39m.\u001b[39msample(n\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\u001b[39m.\u001b[39mplot(kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhist\u001b[39m\u001b[39m'\u001b[39m,bins\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jokes_raw' is not defined"
     ]
    }
   ],
   "source": [
    "jokes_raw[jokes_raw.length <=50].sample(n=10000,random_state=42).plot(kind='hist',bins=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9141e366",
   "metadata": {},
   "source": [
    "## Больше 90% анекдотов имеют длину <= 40, имеет смысл отсечь по 30 или 40 датасет\n",
    "### Как реализую сейчас - размер одного образца датасета других текстов на 30\n",
    "#### UPD. Что было сделано:\n",
    "- Использован эмбединг Navec для шуток и новостных статей\n",
    "- Решено пока эмбедить по id слов, так как иначе - супер много память ( > 25 гб для статей )\n",
    "- В итоге размерность векторов = 30\n",
    "- Размерность выборок: 250к - анекдоты, 200к - статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e30bdd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n"
     ]
    }
   ],
   "source": [
    "print(jokes_raw['length'].quantile(0.95))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1f2228",
   "metadata": {},
   "source": [
    "# Подготовка датасета к использованию классическими моделями\n",
    "### *Идея - с помощью PCA уменьшить размерность каждого образца с 9000 координат до ~ 120 координат на каждый образец*\n",
    "### *Для моделей глубокого обучения планируется использовать весь датасет + делать отдельные dataset + dataloader*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfd7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_raw = pd.read_pickle(r\"C:\\Users\\User\\Documents\\large datasets\\russian_jokes_navec.df\")\n",
    "non_jokes = pd.read_pickle(r\"C:\\Users\\User\\Documents\\large datasets\\padded_news.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2882ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compact(data, target_len ):\n",
    "    from sklearn.decomposition import PCA \n",
    "    '''\n",
    "    Uses PCA to shorten the embedded vectors\n",
    "    Params:\n",
    "    data - your initial vectors of form [[[embed1],[embed2]...]...]\n",
    "    target_len - target len of one data row in the resulting data array. Needed for computing PCA components\n",
    "    Returns:\n",
    "    data in the form [[new_vector1],...] where len(new_vector) = target_len\n",
    "\n",
    "    '''\n",
    "    num_of_c = target_len//len(data[0])\n",
    "    pca = PCA(n_components=num_of_c)\n",
    "    res = []\n",
    "    for i in tqdm(data):\n",
    "        temp_res = pca.fit_transform(i)\n",
    "        temp_res = [j for k in temp_res for j in k] \n",
    "        res.append(temp_res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24be4660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 81662/264133 [01:53<04:20, 700.03it/s]c:\\Users\\User\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:545: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "100%|██████████| 264133/264133 [06:05<00:00, 721.68it/s]\n"
     ]
    }
   ],
   "source": [
    "jokes_pca = preprocess_compact(jokes_raw['embedded_new'],120)\n",
    "non_jokes_pca = preprocess_compact(non_jokes['embedded'],120)\n",
    "non_jokes_pca = pd.DataFrame(non_jokes_pca)\n",
    "jokes_pca_df = pd.DataFrame(jokes_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ff84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target, 1 - joke, 0 - not joke\n",
    "non_jokes_pca['target'] = 0\n",
    "jokes_pca_df['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020efcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_pca_df.to_pickle(r\"C:\\Users\\User\\Documents\\large datasets\\russian_jokes_pca.df\")\n",
    "non_jokes_pca.to_pickle(r\"C:\\Users\\User\\Documents\\large datasets\\russian_news_pca.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc6f38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating balanced dataset for classification\n",
    "common = pd.concat([non_jokes_pca[:100000],jokes_pca_df[:100000]])\n",
    "common = common.reset_index()\n",
    "common.drop('index',axis=1,inplace=True)\n",
    "common.to_pickle(r\"C:\\Users\\User\\Documents\\large datasets\\classic_data.df\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e19c449d",
   "metadata": {},
   "source": [
    "# Итоги:\n",
    "- Все тексты были преобразованы с PCA до размера 120 на экземпляр и сохранены отдельно с таргетами\n",
    "- Был сформирован датасет из 100к шуток и 100к не шуток для классических моделе1\n",
    "- Были сформированы отдельные датасеты с полными эмбеддингами Navec размерности 300 на каждое слово для моделей глубокого обучения + backup\n",
    "- Были сформированы функции и regexp для формирования лемматизированных текстов на русском языке ( из полностью сырых до embedded )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
